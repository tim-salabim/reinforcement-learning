[
  {
    "objectID": "easy21_assigment.html",
    "href": "easy21_assigment.html",
    "title": "Easy21",
    "section": "",
    "text": "In this document we develop a solution for the Easy21 assignment of the course.\nEasy21 is basically a modified version of Black Jack.\n\nRulesEx-1: ImplementationEx-2: Monte-Carlo Control\n\n\n\n\nThe rules outlined in the assignment for the card draw are:\n\nThe game is played with an infinite deck of cards (i.e. cards are sampled with replacement)\nEach draw from the deck results in a value between 1 and 10 (uniformly distributed) with a colour of red (probability 1/3) or black (probability 2/3).\nThere are no aces or picture (face) cards in this game\n\n\nRPython\n\n\n\nvalues = 1:10\ncolors = c(\"red\", \"black\", \"black\")\n\ndrawCard = function(color) {\n  val = sample(values, 1)\n  if (missing(color)) {\n    color = sample(colors, 1)\n  }\n  \n  if (color == \"red\") {\n    val = -val\n  }\n  \n  return(val)\n}\n\ndrawCard(\"black\")\n\n[1] 4\n\ndrawCard(\"red\")\n\n[1] -2\n\nreplicate(10, drawCard())\n\n [1] -8  4 -2  2 10 -7  1  9  5  8\n\n\n\n\n\nimport random\n\nvalues = range(1, 11)\ncolors = [\"red\", \"black\", \"black\"]\n\ndef drawCard(color=None):\n  val = random.sample(values, 1).pop()\n\n  if color is None:\n    color = random.sample(colors, 1).pop()\n  \n  if color == \"red\":\n    val = val * -1\n  \n  return val\n\ndrawCard(\"black\")\n\n6\n\ndrawCard(\"red\")\n\n-10\n\ncards = [drawCard(\"black\")]\nfor i in range(9):\n  cards.append(drawCard())\n\ncards\n\n[4, 8, 7, 7, -1, 10, 10, -5, -10, 1]\n\n\n\n\n\n\n\n\nThe rules for playing of the game are:\n\nAt the start of the game both the player and the dealer draw one black card (fully observed)\nEach turn the player may either stick or hit\nIf the player hits then she draws another card from the deck\nIf the player sticks she receives no further cards\nThe values of the player’s cards are added (black cards) or subtracted (red cards)\n\n\nRPython\n\n\n\nhit = function(cards) {\n  new_card = drawCard()\n  c(cards, new_card)\n}\n\n# start\ncards = drawCard(\"black\")\ncards\n\n[1] 1\n\n# hit\nhit(cards)\n\n[1]  1 -4\n\n\n\n\n\ndef hit(cards):\n  new_card = drawCard()\n  return [cards, new_card]\n\n# start\ncards = drawCard(\"black\")\ncards\n\n9\n\n# hit\nhit(cards)\n\n[9, -9]\n\n\n\n\n\n\n\n\nThe rules for win/lose and associated rewards are:\n\nIf the player’s sum exceeds 21, or becomes less than 1, then she “goes bust” and loses the game (reward -1)\nIf the player sticks then the dealer starts taking turns. The dealer always sticks on any sum of 17 or greater, and hits otherwise. If the dealer goes bust, then the player wins; otherwise, the outcome – win (reward +1), lose (reward -1), or draw (reward 0) – is the player with the largest sum.\n\n\n\n\nPart 1 of the Easy21 Assignment:\n\nYou should write an environment that implements the game Easy21. Specifically, write a function, named \\(step\\), which takes as input a state \\(s\\) (dealer’s first card 1–10 and the player’s sum 1–21), and an action \\(a\\) (hit or stick), and returns a sample of the next state \\(s'\\) (which may be terminal if the game is finished) and reward \\(r\\). We will be using this environment for model-free reinforcement learning, and you should not explicitly represent the transition matrix for the MDP. There is no discounting (\\(\\gamma = 1\\)). You should treat the dealer’s moves as part of the environment, i.e. calling \\(step\\) with a stick action will play out the dealer’s cards and return the final reward and terminal state.\n\n\nRPython\n\n\n\nstep = function(\n    s = c(drawCard(\"black\"), drawCard(\"black\"))\n    , a = c(\"hit\", \"stick\")\n) {\n  a = match.arg(a)\n  r = 0\n  t = FALSE\n  # player hits - add next card\n  if (a == \"hit\") {\n    s[2] = sum(hit(s[2]))\n    if (s[2] &lt; 1 || s[2] &gt; 21) {\n      r = -1\n      t = TRUE\n    }\n    return(\n      list(\n        s = s\n        , r = r\n        , terminated = t\n      )\n    )\n  }\n  \n  # player sticks - play out game\n  if (a == \"stick\") {\n    if (s[1] &gt;= 17) {\n      if (s[1] &lt; 1 || s[1] &gt; 21) {\n        r = 1\n      } else if (s[2] &gt; s[1]) {\n        r = 1\n      } else if (s[1] &gt; s[2]) {\n        r = -1\n      }\n      return(\n        list(\n          s = s\n          , r = r\n          , terminated = TRUE\n        )\n      )\n    } else {\n      s[1] = sum(hit(s[1]))\n      s = step(s, \"stick\")\n      return(s)\n    }\n  }\n}\n\n# play first card and hit\ns1 = step(a = \"hit\")\ns1\n\n$s\n[1]  9 16\n\n$r\n[1] 0\n\n$terminated\n[1] FALSE\n\n# hit one more time then stick and let dealer play out\ns2 = step(s1$s, \"hit\")\nstep(s2$s, \"stick\")\n\n$s\n[1] 20 21\n\n$r\n[1] 1\n\n$terminated\n[1] TRUE\n\n\n\n\n\ndef step(s, a):\n  r = 0\n  \n  if a == \"hit\":\n    s[1] = sum(hit(s[1]))\n    \n    if s[1] &lt; 1 or s[1] &gt; 21:\n      r = -1\n    \n    return {\"s\": s, \"r\": r}\n  \n  if a == \"stick\":\n    if s[0] &gt;= 17:\n      if s[0] &lt; 1 or s[0] &gt; 21:\n        r = 1\n      elif s[1] &gt; s[0]:\n        r = 1\n      elif s[0] &gt; s[1]:\n        r = -1\n        \n      return {\"s\": s, \"r\": r}\n  \n    else:\n      s[0] = sum(hit(s[0]))\n      s = step(s, \"stick\")\n      return s\n\n# play first card and hit\ns1 = step(s = [drawCard(\"black\"), drawCard(\"black\")], a = \"hit\")\ns1\n\n{'s': [8, 5], 'r': 0}\n\n# hit one more time then stick and let dealer play out\ns2 = step(s1[\"s\"], \"hit\")\nstep(s2[\"s\"], \"stick\")\n\n{'s': [17, 1], 'r': -1}\n\n\n\n\n\n\n\n\nApply Monte-Carlo control to Easy21. Initialise the value function to zero. Use a time-varying scalar step-size of \\(\\alpha_{t} = 1/N(s_{t}, a_{t})\\) and an \\(\\epsilon\\)-greedy exploration strategy with \\(\\epsilon_{t} = N_{0}/(N_{0} + N(s_{t}))\\), where \\(N_{0} = 100\\) is a constant, \\(N(s)\\) is the number of times that state s has been visited, and \\(N(s, a)\\) is the number of times that action \\(a\\) has been selected from state \\(s\\). Feel free to choose an alternative value for \\(N_{0}\\), if it helps producing better results. Plot the optimal value function \\(V^{∗} (s) = max_{a} Q^{∗} (s, a)\\) using similar axes to the following figure taken from Sutton and Barto’s Blackjack example.\n\n\n\n\n\nRPython\n\n\n\nalpha = function(Na) {\n  1 / Na\n}\n\nepsilon = function(Ns, N0 = 100) {\n  N0 / (N0 + Ns)\n}\n\ngreedyAction = function(actions, values) {\n  if (length(actions) == 1) {\n    return(actions)\n  }\n  if (Reduce(identical, values)) {\n    return(NULL)\n  }\n  actions[which.max(values)]\n}\n  \nchooseAction = function(epsilon, greedy_action = NULL) {\n  action = sample(c(\"hit\", \"stick\"), 1)\n  if (!is.null(greedy_action)) {\n    action = sample(\n      c(greedy_action, action)\n      , 1\n      , prob = c(max(0, 1 - epsilon), epsilon)\n    )\n  }\n  return(action)\n}\n\nmcUpdate = function(value, reward, Ns, alpha) {\n  value + alpha * (reward - value) / Ns\n}\n\n\n\n\n\n\n\n\nlibrary(data.table)\n\nplayOneIteration = function(DT, state_to_play) {\n  DTin = copy(DT)\n  \n  SDout = DTin[\n    state_dealer == state_to_play[1] & \n      state_player == state_to_play[2], \n  ]\n  \n  if (nrow(SDout) == 0) {\n    stop(\n      sprintf(\n        \"something went wrong, state %s not found\"\n        , state_to_play\n      )\n    )\n  }\n  \n  if (nrow(SDout) &gt; 1) {\n    stop(\"not implemented yet\")\n  }\n  \n  # play next step and update original Na and value\n  s1 = step(SDout[, c(state_dealer, state_player)], SDout[, action])\n  \n  ## FIXME - the value update needs to happen at the end or better outside this function! Only accumulate here, calculate later?\n  SDout[, Na := Na + 1] #[, value := mcUpdate(value, s1$r, Ns, Na)]\n  \n  # does new state s1 already exist in DTin?\n  nrw = nrow(\n    DTin[\n      state_dealer == s1$s[1] & \n        state_player == s1$s[2], \n    ]\n  )\n  \n  # if it does:\n  if (nrw != 0) {\n    stop(\n      sprintf(\n        \"state %s already exists, not implemented yet\"\n        , s1$s\n      )\n    )\n  }\n  \n  # if it doesn't:\n  ## FIXME here\n  \n  df = data.table(\n    state_dealer = s1$s[1]\n    , state_player = s1$s[2]\n    , action = chooseAction(epsilon(0))\n    , Na = 0\n    , Ns = 1\n    , value = s1$r\n    , terminal = s1$terminated\n  )\n  \n  SDreturn = rbind(SDout, df)\n  \n  return(SDreturn)\n  \n}\n\n\nplayers_cards = drawCard(\"black\")\ndealers_card = drawCard(\"black\")\n\ndf = data.table(\n  state_dealer = dealers_card\n  , state_player = players_cards\n  , action = chooseAction(epsilon(0))\n  , Na = 0\n  , Ns = 1\n  , value = 0\n  , terminal = FALSE\n)\n\ndf\n\n   state_dealer state_player action Na Ns value terminal\n1:           10            2    hit  0  1     0    FALSE\n\nnew_df = playOneIteration(\n  df\n  , state_to_play = c(df$state_dealer, df$state_player)\n)\n\nnew_df\n\n   state_dealer state_player action Na Ns value terminal\n1:           10            2    hit  1  1     0    FALSE\n2:           10            3  stick  0  1     0    FALSE\n\nwhile (!any(new_df[, terminal])) {\n  new_df = rbind(\n    new_df[-nrow(new_df), ]\n    , playOneIteration(\n      new_df\n      , new_df[nrow(new_df), c(state_dealer, state_player)]\n    )\n  )\n}\n\nnew_df\n\n   state_dealer state_player action Na Ns value terminal\n1:           10            2    hit  1  1     0    FALSE\n2:           10            3  stick  1  1     0    FALSE\n3:           17            3  stick  0  1    -1     TRUE\n\nn = nrow(new_df)\nvalues = new_df[, value]\n\nfor (i in seq_len(n)) {\n  j = i - 1\n  if (i == 1) {\n    new_df[i, value := mean(tail(values, n))]\n  } else {\n    new_df[i, value := mean(tail(values, -j))]\n  }\n}\n\nprint(new_df)\n\n   state_dealer state_player action Na Ns      value terminal\n1:           10            2    hit  1  1 -0.3333333    FALSE\n2:           10            3  stick  1  1 -0.5000000    FALSE\n3:           17            3  stick  0  1 -1.0000000     TRUE"
  },
  {
    "objectID": "easy21_assigment.html#control-functions",
    "href": "easy21_assigment.html#control-functions",
    "title": "Easy21",
    "section": "",
    "text": "RPython\n\n\n\nalpha = function(Na) {\n  1 / Na\n}\n\nepsilon = function(Ns, N0 = 100) {\n  N0 / (N0 + Ns)\n}\n\ngreedyAction = function(actions, values) {\n  if (length(actions) == 1) {\n    return(actions)\n  }\n  if (Reduce(identical, values)) {\n    return(NULL)\n  }\n  actions[which.max(values)]\n}\n  \nchooseAction = function(epsilon, greedy_action = NULL) {\n  action = sample(c(\"hit\", \"stick\"), 1)\n  if (!is.null(greedy_action)) {\n    action = sample(\n      c(greedy_action, action)\n      , 1\n      , prob = c(max(0, 1 - epsilon), epsilon)\n    )\n  }\n  return(action)\n}\n\nmcUpdate = function(value, reward, Ns, alpha) {\n  value + alpha * (reward - value) / Ns\n}\n\n\n\n\n\n\n\n\nlibrary(data.table)\n\nplayOneIteration = function(DT, state_to_play) {\n  DTin = copy(DT)\n  \n  SDout = DTin[\n    state_dealer == state_to_play[1] & \n      state_player == state_to_play[2], \n  ]\n  \n  if (nrow(SDout) == 0) {\n    stop(\n      sprintf(\n        \"something went wrong, state %s not found\"\n        , state_to_play\n      )\n    )\n  }\n  \n  if (nrow(SDout) &gt; 1) {\n    stop(\"not implemented yet\")\n  }\n  \n  # play next step and update original Na and value\n  s1 = step(SDout[, c(state_dealer, state_player)], SDout[, action])\n  \n  ## FIXME - the value update needs to happen at the end or better outside this function! Only accumulate here, calculate later?\n  SDout[, Na := Na + 1] #[, value := mcUpdate(value, s1$r, Ns, Na)]\n  \n  # does new state s1 already exist in DTin?\n  nrw = nrow(\n    DTin[\n      state_dealer == s1$s[1] & \n        state_player == s1$s[2], \n    ]\n  )\n  \n  # if it does:\n  if (nrw != 0) {\n    stop(\n      sprintf(\n        \"state %s already exists, not implemented yet\"\n        , s1$s\n      )\n    )\n  }\n  \n  # if it doesn't:\n  ## FIXME here\n  \n  df = data.table(\n    state_dealer = s1$s[1]\n    , state_player = s1$s[2]\n    , action = chooseAction(epsilon(0))\n    , Na = 0\n    , Ns = 1\n    , value = s1$r\n    , terminal = s1$terminated\n  )\n  \n  SDreturn = rbind(SDout, df)\n  \n  return(SDreturn)\n  \n}\n\n\nplayers_cards = drawCard(\"black\")\ndealers_card = drawCard(\"black\")\n\ndf = data.table(\n  state_dealer = dealers_card\n  , state_player = players_cards\n  , action = chooseAction(epsilon(0))\n  , Na = 0\n  , Ns = 1\n  , value = 0\n  , terminal = FALSE\n)\n\ndf\n\n   state_dealer state_player action Na Ns value terminal\n1:           10            2    hit  0  1     0    FALSE\n\nnew_df = playOneIteration(\n  df\n  , state_to_play = c(df$state_dealer, df$state_player)\n)\n\nnew_df\n\n   state_dealer state_player action Na Ns value terminal\n1:           10            2    hit  1  1     0    FALSE\n2:           10            3  stick  0  1     0    FALSE\n\nwhile (!any(new_df[, terminal])) {\n  new_df = rbind(\n    new_df[-nrow(new_df), ]\n    , playOneIteration(\n      new_df\n      , new_df[nrow(new_df), c(state_dealer, state_player)]\n    )\n  )\n}\n\nnew_df\n\n   state_dealer state_player action Na Ns value terminal\n1:           10            2    hit  1  1     0    FALSE\n2:           10            3  stick  1  1     0    FALSE\n3:           17            3  stick  0  1    -1     TRUE\n\nn = nrow(new_df)\nvalues = new_df[, value]\n\nfor (i in seq_len(n)) {\n  j = i - 1\n  if (i == 1) {\n    new_df[i, value := mean(tail(values, n))]\n  } else {\n    new_df[i, value := mean(tail(values, -j))]\n  }\n}\n\nprint(new_df)\n\n   state_dealer state_player action Na Ns      value terminal\n1:           10            2    hit  1  1 -0.3333333    FALSE\n2:           10            3  stick  1  1 -0.5000000    FALSE\n3:           17            3  stick  0  1 -1.0000000     TRUE"
  }
]