---
title: "Easy21"
author: "Tim Appelhans"
format: html
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
```

# Reinforcement Learning Assignment: Easy21

In this document we develop a solution for the [Easy21](https://www.davidsilver.uk/wp-content/uploads/2020/03/Easy21-Johannes.pdf) assignment of the course.

Easy21 is basically a modified version of Black Jack.

::: {.panel-tabset}

# Rules

### draw card

The rules outlined in the assignment for the card draw are:

* The game is played with an infinite deck of cards (i.e. cards are sampled with replacement)
* Each draw from the deck results in a value between 1 and 10 (uniformly distributed) with a colour of red (probability 1/3) or black (probability 2/3).
* There are no aces or picture (face) cards in this game

::: {.panel-tabset}

#### R

```{r r-draw}
values = 1:10
colors = c("red", "black", "black")

drawCard = function(color) {
  val = sample(values, 1)
  if (missing(color)) {
    color = sample(colors, 1)
  }
  
  if (color == "red") {
    val = -val
  }
  
  return(val)
}

drawCard("black")
drawCard("red")
replicate(10, drawCard())
```

#### Python

```{python py-draw}
import random

values = range(1, 11)
colors = ["red", "black", "black"]

def drawCard(color=None):
  val = random.sample(values, 1).pop()

  if color is None:
    color = random.sample(colors, 1).pop()
  
  if color == "red":
    val = val * -1
  
  return val

drawCard("black")
drawCard("red")

cards = [drawCard("black")]
for i in range(9):
  cards.append(drawCard())

cards
```

:::

### playing

The rules for playing of the game are:

* At the start of the game both the player and the dealer draw one black card (fully observed)
* Each turn the player may either stick or hit
* If the player hits then she draws another card from the deck
* If the player sticks she receives no further cards
* The values of the player’s cards are added (black cards) or subtracted (red cards)

::: {.panel-tabset}

#### R

```{r r-hit}
hit = function(cards) {
  new_card = drawCard()
  c(cards, new_card)
}

# start
cards = drawCard("black")
cards

# hit
hit(cards)
```

#### Python

```{python py-hit}
def hit(cards):
  new_card = drawCard()
  return [cards, new_card]

# start
cards = drawCard("black")
cards

# hit
hit(cards)
```

:::

### win or lose and rewards

The rules for win/lose and associated rewards are:

* If the player’s sum exceeds 21, or becomes less than 1, then she “goes bust” and loses the game (reward -1)
* If the player sticks then the dealer starts taking turns. The dealer always sticks on any sum of 17 or greater, and hits otherwise. If the dealer goes bust, then the player wins; otherwise, the outcome – win (reward +1), lose (reward -1), or draw (reward 0) – is the player with the largest sum.

# Ex-1: Implementation

Part 1 of the Easy21 Assignment:

> You should write an environment that implements the game Easy21. Specifically, write a function, named $step$, which takes as input a state $s$ (dealer’s first card 1–10 and the player’s sum 1–21), and an action $a$ (hit or stick), and returns a sample of the next state $s'$ (which may be terminal if the game is finished) and reward $r$. We will be using this environment for model-free reinforcement learning, and you should not explicitly represent the transition matrix for the MDP. There is no discounting ($\gamma = 1$). You should treat the dealer’s moves as part of the environment, i.e. calling $step$ with a stick action will play out the dealer’s cards and return the final reward and terminal state.

::: {.panel-tabset}

#### R

```{r r-step}
step = function(
    s = c(drawCard("black"), drawCard("black"))
    , a = c("hit", "stick")
) {
  a = match.arg(a)
  r = 0
  t = FALSE
  # player hits - add next card
  if (a == "hit") {
    s[2] = sum(hit(s[2]))
    if (s[2] < 1 || s[2] > 21) {
      r = -1
      t = TRUE
    }
    return(
      list(
        s = s
        , r = r
        , terminated = t
      )
    )
  }
  
  # player sticks - play out game
  if (a == "stick") {
    if (s[1] >= 17) {
      if (s[1] < 1 || s[1] > 21) {
        r = 1
      } else if (s[2] > s[1]) {
        r = 1
      } else if (s[1] > s[2]) {
        r = -1
      }
      return(
        list(
          s = s
          , r = r
          , terminated = TRUE
        )
      )
    } else {
      s[1] = sum(hit(s[1]))
      s = step(s, "stick")
      return(s)
    }
  }
}

# play first card and hit
s1 = step(a = "hit")
s1

# hit one more time then stick and let dealer play out
s2 = step(s1$s, "hit")
step(s2$s, "stick")
```

#### Python

```{python py-step}
def step(s, a):
  r = 0
  
  if a == "hit":
    s[1] = sum(hit(s[1]))
    
    if s[1] < 1 or s[1] > 21:
      r = -1
    
    return {"s": s, "r": r}
  
  if a == "stick":
    if s[0] >= 17:
      if s[0] < 1 or s[0] > 21:
        r = 1
      elif s[1] > s[0]:
        r = 1
      elif s[0] > s[1]:
        r = -1
        
      return {"s": s, "r": r}
  
    else:
      s[0] = sum(hit(s[0]))
      s = step(s, "stick")
      return s

# play first card and hit
s1 = step(s = [drawCard("black"), drawCard("black")], a = "hit")
s1

# hit one more time then stick and let dealer play out
s2 = step(s1["s"], "hit")
step(s2["s"], "stick")
```

:::

# Ex-2: Monte-Carlo Control

> Apply Monte-Carlo control to Easy21. Initialise the value function to zero. Use a time-varying scalar step-size of $\alpha_{t} = 1/N(s_{t}, a_{t})$ and an $\epsilon$-greedy exploration strategy with $\epsilon_{t} = N_{0}/(N_{0} + N(s_{t}))$, where $N_{0} = 100$ is a constant, $N(s)$ is the number of times that state s has been visited, and $N(s, a)$ is the number of times that action $a$ has been selected from state $s$. Feel free to choose an alternative value for $N_{0}$, if it helps producing better results. Plot the optimal value function $V^{∗} (s) = max_{a} Q^{∗} (s, a)$ using similar axes to the following figure taken from Sutton and Barto’s Blackjack example.

![](sutton_barto_blackjack.png)

## Control functions

::: {.panel-tabset}

#### R

```{r r-mc-funcs}
#| eval: true
#| echo: true

alpha = function(Na) {
  1 / Na
}

epsilon = function(Ns, N0 = 100) {
  N0 / (N0 + Ns)
}

greedyAction = function(actions, values) {
  if (length(actions) == 1) {
    return(actions)
  }
  if (Reduce(identical, values)) {
    return(NULL)
  }
  actions[which.max(values)]
}
  
chooseAction = function(epsilon, greedy_action = NULL) {
  action = sample(c("hit", "stick"), 1)
  if (!is.null(greedy_action)) {
    action = sample(
      c(greedy_action, action)
      , 1
      , prob = c(max(0, 1 - epsilon), epsilon)
    )
  }
  return(action)
}

mcUpdate = function(value, reward, Ns, alpha) {
  value + alpha * (reward - value) / Ns
}
```

#### Python

```{python py-mc-funcs}

```

:::

```{r r-control}
#| eval: true
#| echo: true

library(data.table)

playOneIteration = function(DT, state_to_play) {
  DTin = copy(DT)
  
  SDout = DTin[
    state_dealer == state_to_play[1] & 
      state_player == state_to_play[2], 
  ]
  
  if (nrow(SDout) == 0) {
    stop(
      sprintf(
        "something went wrong, state %s not found"
        , state_to_play
      )
    )
  }
  
  if (nrow(SDout) > 1) {
    stop("not implemented yet")
  }
  
  # play next step and update original Na and value
  s1 = step(SDout[, c(state_dealer, state_player)], SDout[, action])
  
  ## FIXME - the value update needs to happen at the end or better outside this function! Only accumulate here, calculate later?
  SDout[, Na := Na + 1] #[, value := mcUpdate(value, s1$r, Ns, Na)]
  
  # does new state s1 already exist in DTin?
  nrw = nrow(
    DTin[
      state_dealer == s1$s[1] & 
        state_player == s1$s[2], 
    ]
  )
  
  # if it does:
  if (nrw != 0) {
    stop(
      sprintf(
        "state %s already exists, not implemented yet"
        , s1$s
      )
    )
  }
  
  # if it doesn't:
  ## FIXME here
  
  df = data.table(
    state_dealer = s1$s[1]
    , state_player = s1$s[2]
    , action = chooseAction(epsilon(0))
    , Na = 0
    , Ns = 1
    , value = s1$r
    , terminal = s1$terminated
  )
  
  SDreturn = rbind(SDout, df)
  
  return(SDreturn)
  
}


players_cards = drawCard("black")
dealers_card = drawCard("black")

df = data.table(
  state_dealer = dealers_card
  , state_player = players_cards
  , action = chooseAction(epsilon(0))
  , Na = 0
  , Ns = 1
  , value = 0
  , terminal = FALSE
)

df

new_df = playOneIteration(
  df
  , state_to_play = c(df$state_dealer, df$state_player)
)

new_df

while (!any(new_df[, terminal])) {
  new_df = rbind(
    new_df[-nrow(new_df), ]
    , playOneIteration(
      new_df
      , new_df[nrow(new_df), c(state_dealer, state_player)]
    )
  )
}

new_df

n = nrow(new_df)
values = new_df[, value]

for (i in seq_len(n)) {
  j = i - 1
  if (i == 1) {
    new_df[i, value := mean(tail(values, n))]
  } else {
    new_df[i, value := mean(tail(values, -j))]
  }
}

print(new_df)

```

:::
