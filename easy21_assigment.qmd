---
title: "Easy21"
author: "Tim Appelhans"
format: html
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
```

# Reinforcement Learning Assignment: Easy21

In this document we develop a solution for the [Easy21](https://www.davidsilver.uk/wp-content/uploads/2020/03/Easy21-Johannes.pdf) assignment of the course.

Easy21 is basically a modified version of Black Jack.

::: {.panel-tabset}

# Rules

### draw card

The rules outlined in the assignment for the card draw are:

* The game is played with an infinite deck of cards (i.e. cards are sampled with replacement)
* Each draw from the deck results in a value between 1 and 10 (uniformly distributed) with a colour of red (probability 1/3) or black (probability 2/3).
* There are no aces or picture (face) cards in this game

::: {.panel-tabset}

#### R

```{r r-draw}
card_values = 1:10
card_colors = c("red", "black", "black")

drawCard = function(color) {
  val = sample(card_values, 1)
  if (missing(color)) {
    color = sample(card_colors, 1)
  }
  
  if (color == "red") {
    val = -val
  }
  
  return(val)
}

drawCard("black")
drawCard("red")
replicate(10, drawCard())
```

#### Python

```{python py-draw}
import random

values = range(1, 11)
colors = ["red", "black", "black"]

def drawCard(color=None):
  val = random.sample(values, 1).pop()

  if color is None:
    color = random.sample(colors, 1).pop()
  
  if color == "red":
    val = val * -1
  
  return val

drawCard("black")
drawCard("red")

cards = [drawCard("black")]
for i in range(9):
  cards.append(drawCard())

cards
```

:::

### playing

The rules for playing of the game are:

* At the start of the game both the player and the dealer draw one black card (fully observed)
* Each turn the player may either stick or hit
* If the player hits then she draws another card from the deck
* If the player sticks she receives no further cards
* The values of the player’s cards are added (black cards) or subtracted (red cards)

::: {.panel-tabset}

#### R

```{r r-hit}
hit = function(cards) {
  new_card = drawCard()
  c(cards, new_card)
}

# start
cards = drawCard("black")
cards

# hit
hit(cards)
```

#### Python

```{python py-hit}
def hit(cards):
  new_card = drawCard()
  return [cards, new_card]

# start
cards = drawCard("black")
cards

# hit
hit(cards)
```

:::

### win or lose and rewards

The rules for win/lose and associated rewards are:

* If the player’s sum exceeds 21, or becomes less than 1, then she “goes bust” and loses the game (reward -1)
* If the player sticks then the dealer starts taking turns. The dealer always sticks on any sum of 17 or greater, and hits otherwise. If the dealer goes bust, then the player wins; otherwise, the outcome – win (reward +1), lose (reward -1), or draw (reward 0) – is the player with the largest sum.

# Ex-1: Implementation

Part 1 of the Easy21 Assignment:

> You should write an environment that implements the game Easy21. Specifically, write a function, named $step$, which takes as input a state $s$ (dealer’s first card 1–10 and the player’s sum 1–21), and an action $a$ (hit or stick), and returns a sample of the next state $s'$ (which may be terminal if the game is finished) and reward $r$. We will be using this environment for model-free reinforcement learning, and you should not explicitly represent the transition matrix for the MDP. There is no discounting ($\gamma = 1$). You should treat the dealer’s moves as part of the environment, i.e. calling $step$ with a stick action will play out the dealer’s cards and return the final reward and terminal state.

::: {.panel-tabset}

#### R

```{r r-step}
step = function(
    s = c(drawCard("black"), drawCard("black"))
    , a = c("hit", "stick")
) {
  a = match.arg(a)
  r = 0
  t = FALSE
  # player hits - add next card
  if (a == "hit") {
    s[2] = sum(hit(s[2]))
    if (s[2] < 1 || s[2] > 21) {
      r = -1
      t = TRUE
    }
    return(
      list(
        s = s
        , r = r
        , terminated = t
      )
    )
  }
  
  # player sticks - play out game
  if (a == "stick") {
    if (s[1] >= 17) {
      if (s[1] < 1 || s[1] > 21) {
        r = 1
      } else if (s[2] > s[1]) {
        r = 1
      } else if (s[1] > s[2]) {
        r = -1
      }
      return(
        list(
          s = s
          , r = r
          , terminated = TRUE
        )
      )
    } else {
      s[1] = sum(hit(s[1]))
      s = step(s, "stick")
      return(s)
    }
  }
}

# play first card and hit
s1 = step(a = "hit")
s1

# hit one more time then stick and let dealer play out
s2 = step(s1$s, "hit")
step(s2$s, "stick")
```

#### Python

```{python py-step}
def step(s, a):
  r = 0
  
  if a == "hit":
    s[1] = sum(hit(s[1]))
    
    if s[1] < 1 or s[1] > 21:
      r = -1
    
    return {"s": s, "r": r}
  
  if a == "stick":
    if s[0] >= 17:
      if s[0] < 1 or s[0] > 21:
        r = 1
      elif s[1] > s[0]:
        r = 1
      elif s[0] > s[1]:
        r = -1
        
      return {"s": s, "r": r}
  
    else:
      s[0] = sum(hit(s[0]))
      s = step(s, "stick")
      return s

# play first card and hit
s1 = step(s = [drawCard("black"), drawCard("black")], a = "hit")
s1

# hit one more time then stick and let dealer play out
s2 = step(s1["s"], "hit")
step(s2["s"], "stick")
```

:::

# Ex-2: Monte-Carlo Control

> Apply Monte-Carlo control to Easy21. Initialise the value function to zero. Use a time-varying scalar step-size of $\alpha_{t} = 1/N(s_{t}, a_{t})$ and an $\epsilon$-greedy exploration strategy with $\epsilon_{t} = N_{0}/(N_{0} + N(s_{t}))$, where $N_{0} = 100$ is a constant, $N(s)$ is the number of times that state s has been visited, and $N(s, a)$ is the number of times that action $a$ has been selected from state $s$. Feel free to choose an alternative value for $N_{0}$, if it helps producing better results. Plot the optimal value function $V^{∗} (s) = max_{a} Q^{∗} (s, a)$ using similar axes to the following figure taken from Sutton and Barto’s Blackjack example.

![](sutton_barto_blackjack.png)

## Control functions

::: {.panel-tabset}

#### R

```{r r-mc-funcs}
#| eval: true
#| echo: true

alpha = function(Na) {
  1 / Na
}

epsilon = function(Ns, N0 = 100) {
  N0 / (N0 + Ns)
}

greedyAction = function(actions, values) {
  if (length(actions) == 1) {
    return(actions)
  }
  if (Reduce(identical, values)) {
    return(NULL)
  }
  actions[which.max(values)]
}
  
chooseAction = function(epsilon, greedy_action = NULL) {
  action = sample(c("hit", "stick"), 1)
  if (!is.null(greedy_action)) {
    action = sample(
      c(greedy_action, action)
      , 1
      , prob = c(max(0, 1 - epsilon), epsilon)
    )
  }
  return(action)
}

mcUpdate = function(value, reward, Ns, alpha) {
  value + alpha * (reward - value) / Ns
}

# isPlayable = function(s) {
#   s[1] <= 10 & !(any(s < 1) || any(s > 21))
# }
```

#### Python

```{python py-mc-funcs}

```

:::

```{r r-control}
#| eval: false
#| echo: false

library(data.table)

initiateTable = function(
    state
    , action
    , Na = 0
    , Ns = 1
    , reward = 0
    , value = 0
    , terminal = FALSE
) {
  data.table(
    state_dealer = state[1]
    , state_player = state[2]
    , action = action
    , Na = Na
    , Ns = Ns
    , reward = reward
    , value = value
    , terminal = terminal
  )
}

mergeTables = function(DT1, DT2) {
  DTout = merge(
    DT1
    , DT2
    , by = c("state_dealer", "state_player", "action")
    , all = TRUE
  )
  
  cols = grep(".x$|.y$", names(DTout), value = TRUE)
  
  DTout[
    , c("Na", "Ns", "reward", "value", "terminal") := list(
      sum(c(Na.x, Na.y), na.rm = TRUE)
      , sum(c(Ns.x, Ns.y), na.rm = TRUE)
      , sum(c(reward.x, reward.y), na.rm = TRUE)
      , value = mean(c(value.x, value.y), na.rm = TRUE)
      , as.logical(max(c(terminal.x, terminal.y), na.rm = TRUE))
    )
    , by = 1:nrow(DTout)
  ][
    , (cols) := NULL
  ]
}

updatePlayedStateAction = function(DT, state, act, r) {
  DTout = copy(DT)
  DTout = DTout[
    state_dealer == state[1] &
      state_player == state[2] &
      action == act
    , c("Na", "reward") := list(Na + 1, reward + r)
  ][
    , value := mcUpdate(value, reward, Ns, alpha(Na))
  ]
  return(DTout)
}

stateAction = function(DT, crd_dlr, sum_plr) {
  DTsbs = copy(DT)
  SDstate = subset(
    DTsbs
    , state_dealer == crd_dlr &
      state_player == sum_plr
  )
  
  # if there are more than 2 rows of state, raise error
  if (nrow(SDstate) > 2) {
    stop(
      sprintf(
        "I see state %s more than twice, something is not quite correct"
        , paste(dealers_card, players_sum, sep = ", ")
      )
    )
  }
  
  # initiate new random action
  act = chooseAction(epsilon(0))
  
  # if state has been played before, choose epsilon greedy action
  if (nrow(SDstate) > 0) {
    act = chooseAction(
      epsilon(sum(SDstate[, Ns]))
      , greedyAction(SDstate[, action], SDstate[, value])
    )
  }
  
  return(act)
}

stateActionMetrics = function(DT, crd_dlr, sum_plr) {
  DTsbs = copy(DT)
  SDstate = subset(
    DTsbs
    , state_dealer == crd_dlr &
      state_player == sum_plr
  )
  
  # if there are more than 2 rows of state, raise error
  if (nrow(SDstate) > 2) {
    stop(
      sprintf(
        "I see state %s more than twice, something is not quite correct"
        , paste(dealers_card, players_sum, sep = ", ")
      )
    )
  }
  
  # initiate new random action
  act = chooseAction(epsilon(0))
  
  # if state has been played before, choose epsilon greedy action
  if (nrow(SDstate) > 0) {
    act = chooseAction(
      epsilon(sum(SDstate[, Ns]))
      , greedyAction(SDstate[, action], SDstate[, value])
    )
  }
  
  SDstateaction = subset(
    SDstate
    , action == act
  )
  
  if (nrow(SDstateaction) == 0) {
    Ns = 1
    Na = 0
    reward = 0
    value = 0
    terminal = FALSE
  } else {
    Ns = SDstateaction[, Ns]
    Na = SDstateaction[, Na]
    reward = SDstateaction[, reward]
    value = SDstateaction[, value]
    terminal = SDstateaction[, terminal]
  }
  
  return(
    list(
      action = act
      , Ns = Ns
      , Na = Na
      , reward = reward
      , value = value
      , terminal = terminal
    )
  )
}

# play game ----

# FIXME: there's some problem with counting Ns & Na somewhere...

# initial card draw
for (i in 1:1000) {
# while (min(dat$Ns) < 100) {
  print(i)
  # print(min(dat$Ns))
  dealers_card = drawCard("black")
  players_sum = drawCard("black")
  
  # choose (random) action
  if (exists("dat")) {
    mtrcs = stateActionMetrics(dat, dealers_card, players_sum)
    act = mtrcs$action
  } else {
    act = chooseAction(epsilon(0))
  }
  act
  
  # FIXME here: need to check if state-action already exists!
  # initial table (only at first play of game)
  df_episode = initiateTable(
    state = c(dealers_card, players_sum)
    , action = mtrcs$act
    , Na = mtrcs$Na
    , Ns = mtrcs$Ns
    , reward = mtrcs$reward
    , value = mtrcs$value
    , terminal = mtrcs$terminal
  )
  df_episode
  
  # play state-action
  s1 = step(c(dealers_card, players_sum), act)
  s1
  
  # update played state-action according to gained reward
  df_episode = updatePlayedStateAction(
    df_episode
    , state = c(dealers_card, players_sum)
    , act = act
    , r = s1$r
  )
  df_episode
  df_episode
  
  # if episode not terminated, update (dealers and) players card sum
  while (!s1$terminated) {
    dealers_card = s1$s[1]
    players_sum = s1$s[2]
    
    # determine action to take
    # check if state has been visited before
    SDstate = subset(
      df_episode
      , state_dealer == dealers_card &
        state_player == players_sum
    )
    
    # if there are more than 2 rows of state, raise error 
    if (nrow(SDstate) > 2) {
      stop(
        sprintf(
          "I see state %s more than twice, something is not quite correct"
          , paste(dealers_card, players_sum, sep = ", ")
        )
      )
    }
    
    # initiate new random action
    act = chooseAction(epsilon(0))
    
    # if state has been played before, choose epsilon greedy action
    if (nrow(SDstate) > 0) {
      act = chooseAction(
        epsilon(sum(SDstate[, Ns]))
        , greedyAction(SDstate[, action], SDstate[, value])
      )
    }
    
    # check if state-action has been visited before
    SDstateaction = subset(
      SDstate
      , action == act
    )
    
    # if state-action not seen before, add to table
    if (nrow(SDstateaction) == 0) {
      df_episode = rbind(
        df_episode
        , initiateTable(c(dealers_card, players_sum), act)
      )
    } else {
      df_episode[
        state_dealer == dealers_card &
          state_player == players_sum &
          action == action
        , Ns := Ns + 1
      ]
      # stop(
      #   sprintf(
      #     "state-action %s already exists. Not implemented yet"
      #     , paste(c(dealers_card, players_sum, act), collapse = ", ")
      #   )
      # )
    }
    
    s1 = step(c(dealers_card, players_sum), act)
    df_episode = updatePlayedStateAction(
      df_episode
      , state = c(dealers_card, players_sum)
      , act = act
      , r = s1$r
    )
  }
  
  if (s1$terminated) {
    n = nrow(df_episode)
    values = df_episode[, value]
    
    for (i in seq_len(n)) {
      j = i - 1
      if (i == 1) {
        df_episode[i, value := mean(tail(values, n))]
      } else {
        df_episode[i, value := mean(tail(values, -j))]
      }
    }
  }
  
  # print(df_episode)
  # print(df_episode)
  
  if (!exists("dat")) {
    dat = df_episode
  } else {
    dat = mergeTables(dat, df_episode)
  }
  dat
  dat
}

summary(dat)

DTfinal = dat[
  , .(
    value = max(value)
    , action = action[which.max(value)]
    , Ns = sum(Ns)
    , n = .N
  )
  , by = c("state_dealer", "state_player")
]

lattice::levelplot(
  value ~ state_player * state_dealer
  , data = DTfinal
  , col.regions = hcl.colors(1000, "Purple-Green")
  , at = seq(-1, 1, length.out = 40)
)

lattice::levelplot(
  as.factor(action) ~ state_dealer * state_player
  , data = DTfinal
)


lattice::wireframe(value ~ state_dealer * state_player, data = DTfinal)




# iter = function(state, action) {
#   DT = copy(D)
#   s1 = step(DT[, c(state_dealer, state_player)], DT[, action])
#   s1
#   
#   DT[
#     # state_dealer == dealers_card &
#     #   state_player == players_cards
#     , c("Na", "reward") := list(Na + 1, reward + s1$r)
#   ]
#   
#   
#   act = chooseAction(epsilon(0))
#   Na = 0
#   Ns = 0
#   val = 0
#   
#   SD = DT[
#     state_dealer == s1$s[1] &
#       state_player == s1$s[2]
#   ]
#   
#   if (nrow(SD) > 0) {
#     act = chooseAction(
#       epsilon(sum(SD[, Ns]))
#       , greedyAction(SD[, action], SD[, value])
#     )
#     if (nrow(SD) == 2) {
#       Na = SD[action == act, Na]
#       Ns = SD[action == act, Ns]
#       val = SD[action == act, value]
#     }
#   }
#   
#   s1df = data.table(
#     state_dealer = s1$s[1]
#     , state_player = s1$s[2]
#     , action = act
#     , Na = Na
#     , Ns = Ns + 1
#     , reward = s1$r
#     , value = val
#     , terminal = s1$terminated
#   )
#   
#   DTout = list(
#     played_state = DT
#     , new_state = s1df
#   )
#   
#   return(DTout)
# }

# jnk = iter(df)
# jnk
# 
# df = updatePlayedState(df, jnk[[1]])
# df
# 
# while (!any(sapply(jnk, "[[", "terminal"))) {
#   ln = length(jnk)
#   jnk = append(
#     head(jnk, ln-1)
#     , iter(
#       jnk[[ln]]
#     )
#   )
#   #browser()
# }
# 
# jnk = rbindlist(jnk)[terminal == FALSE]
# jnk = jnk[
#   , value := mcUpdate(
#     value
#     , reward
#     , Ns
#     , alpha(Na)
#   )
#   , by = 1:nrow(jnk)
# ]
# 
# jnk
# 
# n = nrow(jnk)
# values = jnk[, value]
# 
# for (i in seq_len(n)) {
#   j = i - 1
#   if (i == 1) {
#     jnk[i, value := mean(tail(values, n))]
#   } else {
#     jnk[i, value := mean(tail(values, -j))]
#   }
# }
# 
# jnk
# jnk

## FIXME here: running an episode
### 1. draw cards
### 2. check if state already exists
###   a. if no, run iteration & update reward
###   b. if yes, update Ns, Na, and choose (greedy) action & run iteration
###   c. update reward and value of state-action
### 3. repeat 2. until terminal state
### 4. update value with mcUpdate row-wise
### 5. update value as mean return



# playOneIteration = function(DT, state_to_play) {
#   DTin = copy(DT)
#   
#   if (anyDuplicated(DTin)) {
#     browser()
#   }
#   
#   # extract relevant states from DT
#   SDout = DTin[
#     state_dealer == state_to_play[1] & 
#       state_player == state_to_play[2]
#   ]
#   
#   if (nrow(SDout) > 1) {
#     browser()
#     stop("more than 1 row, not implemented yet")
#   }
#   
#   if (nrow(SDout) == 0) {
#     SDout = initiateTable(state_to_play)
#     # stop(
#     #   sprintf(
#     #     "something went wrong, state %s not found"
#     #     , paste(state_to_play, collapse = ", ")
#     #   )
#     # )
#   }
#   
#   act = SDout[, action]
#   
#   # play next step and update original Na and reward
#   s1 = step(SDout[, c(state_dealer, state_player)], act)
#   SDout[, c("Na", "reward") := list(Na + 1, s1$r)] 
#   
#   # does new state s1 already exist in DTin?
#   nrw = nrow(
#     DTin[
#       state_dealer == s1$s[1] & 
#         state_player == s1$s[2], 
#     ]
#   )
#   
#   # if it does:
#   if (nrw != 0) {
#     if (nrw == 1) {
#       SD = DTin[
#         state_dealer == s1$s[1] & 
#           state_player == s1$s[2]
#       ]
#       Ns = sum(SD[, Ns])
#       act = chooseAction(epsilon(Ns), greedyAction(SD[, action], SD[, value]))
#       SD[action == act, Ns := Ns + 1]
#       
#       tst = merge(
#         DTin
#         , SD
#         , by = c("state_dealer", "state_player", "action")
#         , all = TRUE
#       )
#       
#       cols = grep(".x$|.y$", names(tst), value = TRUE)
#       
#       tst[
#         , c("Na", "Ns", "reward", "value", "terminal") := list(
#           sum(c(Na.x, Na.y), na.rm = TRUE)
#           , sum(c(Ns.x, Ns.y), na.rm = TRUE)
#           , sum(c(reward.x, reward.y), na.rm = TRUE)
#           , value = mean(c(value.x, value.y), na.rm = TRUE)
#           , FALSE
#         )
#         , by = 1:nrow(tst)
#       ][
#         , (cols) := NULL
#       ]
# 
#       return(tst)
#     }
#     
#     if (nrw == 2) {
#       stop(
#         sprintf(
#           "state %s already exists, not implemented yet"
#           , paste(s1$s, collapse = ", ")
#         )
#       )
#     }
#   } else {
#     
#     # if it doesn't:
#     df = data.table(
#       state_dealer = s1$s[1]
#       , state_player = s1$s[2]
#       , action = chooseAction(epsilon(0))
#       , Na = 0
#       , Ns = 1
#       , reward = 0
#       , value = s1$r
#       , terminal = s1$terminated
#     )
#     
#     SDreturn = rbind(SDout, df)
#     return(SDreturn)
#   }
#   
# }
# 
# 
# playEpisonde = function(DT, state_to_play) {
#   DTin = copy(DT)
#   if (anyDuplicated(DTin)) {
#     browser()
#   }
#   DTout = playOneIteration(
#     DTin
#     , state_to_play = state_to_play
#     # , state_to_play = c(DT$state_dealer, DT$state_player)
#   )
#   
#   while (!any(DTout[, terminal])) {
#     DTout = rbind(
#       DTout[-nrow(DTout), ]
#       , playOneIteration(
#         DTout
#         , DTout[nrow(DTout), c(state_dealer, state_player)]
#       )
#     )
#   }
#   
#   DTout = DTout[terminal == FALSE, ]
#   DTout
#   
#   DTout = DTout[
#     , value := mcUpdate(
#       value
#       , reward
#       , Ns
#       , alpha(Na)
#     )
#     , by = 1:nrow(DTout)
#   ]
#   
#   n = nrow(DTout)
#   values = DTout[, value]
#   
#   for (i in seq_len(n)) {
#     j = i - 1
#     if (i == 1) {
#       DTout[i, value := mean(tail(values, n))]
#     } else {
#       DTout[i, value := mean(tail(values, -j))]
#     }
#   }
#   
#   return(DTout)
# }
# 
# 
# 
# players_cards = drawCard("black")
# dealers_card = drawCard("black")
# 
# df = initiateTable(c(dealers_card, players_cards))
# df
# 
# epi = playEpisonde(df, c(df$state_dealer, df$state_player))
# epi
# 
# for (i in 1:100) {
#   print(i)
#   new_state = c(drawCard("black"), drawCard("black"))
#   new_epi = playEpisonde(epi, new_state)
#   
#   epi = merge(
#     epi
#     , new_epi
#     , by = c("state_dealer", "state_player", "action")
#     , all = TRUE
#   )
#   
#   cols = grep(".x$|.y$", names(epi), value = TRUE)
#   
#   epi[
#     , c("Na", "Ns", "reward", "value", "terminal") := list(
#       sum(c(Na.x, Na.y), na.rm = TRUE)
#       , sum(c(Ns.x, Ns.y), na.rm = TRUE)
#       , sum(c(reward.x, reward.y), na.rm = TRUE)
#       , value = mean(c(value.x, value.y), na.rm = TRUE)
#       , FALSE
#     )
#     , by = 1:nrow(epi)
#   ][
#     , (cols) := NULL
#   ]
#   setorder(epi, state_dealer, state_player, action)
# }
# epi
# 
# 
# jnk = lapply(
#   1:10
#   , \(i) {
#     epi = rbind(
#       epi
#       , playEpisonde(epi, c(drawCard("black"), drawCard("black")))
#     )
#   }
# )
# 
# jnk = playEpisonde(df, c(df$state_dealer, df$state_player))
# print(jnk)
# 
# jnk2 = rbind(
#   jnk
#   , playEpisonde(jnk, c(drawCard("black"), drawCard("black")))
# )
# print(jnk2)
# 
# tst = merge(
#   jnk
#   , jnk2
#   , by = c("state_dealer", "state_player", "action")
#   , all = TRUE
# )
# 
# cols = grep(".x$|.y$", names(tst), value = TRUE)
# 
# tst[
#   , c("Na", "Ns", "reward", "value", "terminal") := list(
#     sum(c(Na.x, Na.y), na.rm = TRUE)
#     , sum(c(Ns.x, Ns.y), na.rm = TRUE)
#     , sum(c(reward.x, reward.y), na.rm = TRUE)
#     , value = mean(c(value.x, value.y), na.rm = TRUE)
#     , FALSE
#   )
#   , by = 1:nrow(tst)
# ][
#   , (cols) := NULL
# ]
# 
# print(tst)
```

:::
